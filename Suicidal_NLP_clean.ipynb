{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing/importing Libraries"
      ],
      "metadata": {
        "id": "arxehG4Gb49C"
      },
      "id": "arxehG4Gb49C"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "uQfR5ZLB0G3i"
      },
      "id": "uQfR5ZLB0G3i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e18b23",
      "metadata": {
        "id": "b8e18b23"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Class labels to Numerical Value"
      ],
      "metadata": {
        "id": "BvC-sodqcC3t"
      },
      "id": "BvC-sodqcC3t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295962dc",
      "metadata": {
        "id": "295962dc"
      },
      "outputs": [],
      "source": [
        "def extractList(df):\n",
        "    df.loc[df[\"Post\"].str.endswith(\"]\") == False, \"Post\"] = df.loc[df[\"Post\"].str.endswith(\"]\") == False, \"Post\"] + \"']\"\n",
        "    df[\"Post\"] = df[\"Post\"].apply(lambda x: ast.literal_eval(x))\n",
        "    df[\"Post\"] = df[\"Post\"].apply(lambda x: \" \".join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162f3966",
      "metadata": {
        "id": "162f3966"
      },
      "outputs": [],
      "source": [
        "label_conversion = {\"Supportive\": 0,\n",
        "                    \"Indicator\": 1,\n",
        "                    \"Ideation\": 2,\n",
        "                    \"Behavior\": 3,\n",
        "                    \"Attempt\": 4}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Data From GitHub and Loading"
      ],
      "metadata": {
        "id": "9bpVd6rmcJFn"
      },
      "id": "9bpVd6rmcJFn"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hrmoradi/Workshop_data # goes to your cotent folder"
      ],
      "metadata": {
        "id": "pTXt9mHWjh4g"
      },
      "id": "pTXt9mHWjh4g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bedf43d",
      "metadata": {
        "id": "7bedf43d"
      },
      "outputs": [],
      "source": [
        "dat = pd.read_csv(\"/content/Workshop_data/500Reddit.txt\")\n",
        "dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e30bf51",
      "metadata": {
        "id": "5e30bf51"
      },
      "outputs": [],
      "source": [
        "# Rename Labels to integers in order of intensity\n",
        "dat = dat.replace({\"Label\":label_conversion})\n",
        "# Extract list of posts from string of list and concatenate together\n",
        "extractList(dat)\n",
        "dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e7c247",
      "metadata": {
        "id": "61e7c247"
      },
      "outputs": [],
      "source": [
        "num_labels = dat[\"Label\"].nunique()\n",
        "num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90ee253",
      "metadata": {
        "id": "c90ee253"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dat[\"Post\"], dat[\"Label\"], test_size=0.33, random_state=42, stratify=dat[\"Label\"], shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text To verctor Representation"
      ],
      "metadata": {
        "id": "0XYP4kN0cZPQ"
      },
      "id": "0XYP4kN0cZPQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1611cf8",
      "metadata": {
        "id": "d1611cf8"
      },
      "outputs": [],
      "source": [
        "# Import BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969e74cc",
      "metadata": {
        "id": "969e74cc"
      },
      "outputs": [],
      "source": [
        "X_train_input = tokenizer(X_train.values.tolist(), max_length = 512, truncation=\"longest_first\", padding=\"max_length\")\n",
        "X_test_input = tokenizer(X_test.values.tolist(), max_length = 512, truncation=\"longest_first\", padding=\"max_length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fcc3502",
      "metadata": {
        "id": "8fcc3502"
      },
      "outputs": [],
      "source": [
        "train_input_ids = np.asarray(X_train_input[\"input_ids\"])\n",
        "train_att_mask = np.asarray(X_train_input[\"attention_mask\"])\n",
        "\n",
        "test_input_ids = np.asarray(X_test_input[\"input_ids\"])\n",
        "test_att_mask = np.asarray(X_test_input[\"attention_mask\"])\n",
        "\n",
        "y_train = np.asarray(y_train)\n",
        "y_test = np.asarray(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54790e5d",
      "metadata": {
        "id": "54790e5d"
      },
      "source": [
        "# Freezing transformer weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9291373",
      "metadata": {
        "scrolled": true,
        "id": "c9291373"
      },
      "outputs": [],
      "source": [
        "model_frozen = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab52c153",
      "metadata": {
        "id": "ab52c153"
      },
      "outputs": [],
      "source": [
        "model_frozen.bert.trainable = False\n",
        "model_frozen.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "model_frozen.compile(optimizer=Adam(learning_rate=1e-5), \n",
        "                      loss=loss,\n",
        "                      metrics=metrics)"
      ],
      "metadata": {
        "id": "sHl8zY3e0Zvr"
      },
      "id": "sHl8zY3e0Zvr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a7e975b",
      "metadata": {
        "id": "2a7e975b"
      },
      "outputs": [],
      "source": [
        "model_frozen.fit(x=[train_input_ids, train_att_mask], y=y_train, epochs=15, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since TFBertForSequenceClassification returns logits, we must first convert it to probabilities using softmax."
      ],
      "metadata": {
        "id": "tVSAYXO-HSTZ"
      },
      "id": "tVSAYXO-HSTZ"
    },
    {
      "cell_type": "code",
      "source": [
        "output = model_frozen.predict([test_input_ids, test_att_mask])\n",
        "softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "preds = softmax(output.logits)\n",
        "pred_labels = preds.numpy().argmax(axis=1)"
      ],
      "metadata": {
        "id": "DvWnUaiB4YyQ"
      },
      "id": "DvWnUaiB4YyQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Classification Report')\n",
        "print(classification_report(y_test,pred_labels))"
      ],
      "metadata": {
        "id": "LKfHJpxR5pd7"
      },
      "id": "LKfHJpxR5pd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, preds, multi_class='ovr', average='macro')"
      ],
      "metadata": {
        "id": "t-k4Vrt3GP4_"
      },
      "id": "t-k4Vrt3GP4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unfreeze transformer weights"
      ],
      "metadata": {
        "id": "osTphW4S6fWX"
      },
      "id": "osTphW4S6fWX"
    },
    {
      "cell_type": "code",
      "source": [
        "model_unfrozen = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = num_labels)"
      ],
      "metadata": {
        "id": "x__Y5NIC6e4-"
      },
      "id": "x__Y5NIC6e4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_unfrozen.bert.trainable = True # Default value set to true\n",
        "model_unfrozen.summary()"
      ],
      "metadata": {
        "id": "t-kDDAo66roc"
      },
      "id": "t-kDDAo66roc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "model_unfrozen.compile(optimizer=Adam(learning_rate=1e-5), \n",
        "                      loss=loss,\n",
        "                      metrics=metrics)"
      ],
      "metadata": {
        "id": "WJ8s6fBx6rtY"
      },
      "id": "WJ8s6fBx6rtY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_unfrozen.fit(x=[train_input_ids, train_att_mask], y=y_train, epochs=10, batch_size=4)"
      ],
      "metadata": {
        "id": "imHkCsFk66Au"
      },
      "id": "imHkCsFk66Au",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since TFBertForSequenceClassification returns logits, we must first convert it to probabilities using softmax."
      ],
      "metadata": {
        "id": "dxDlWdTbHjYp"
      },
      "id": "dxDlWdTbHjYp"
    },
    {
      "cell_type": "code",
      "source": [
        "output = model_unfrozen.predict([test_input_ids, test_att_mask])\n",
        "softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "preds = softmax(output.logits)\n",
        "pred_labels = preds.numpy().argmax(axis=1)"
      ],
      "metadata": {
        "id": "OobaofR27AHE"
      },
      "id": "OobaofR27AHE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Classification Report')\n",
        "print(classification_report(y_test,pred_labels))"
      ],
      "metadata": {
        "id": "d3NwEsdE7CqK"
      },
      "id": "d3NwEsdE7CqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, preds, multi_class='ovr', average='macro')"
      ],
      "metadata": {
        "id": "sLVXshijFmPm"
      },
      "id": "sLVXshijFmPm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create your own traditional model"
      ],
      "metadata": {
        "id": "YD3ix-Cmfr9M"
      },
      "id": "YD3ix-Cmfr9M"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# # uncomment for IMDB dataset # 3 lines below\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=512)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=512)\n",
        "\n",
        "# Input for variable-length sequences of integers\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# # uncomment vectorization for suicidal datae set\n",
        "# vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "#     standardize='lower_and_strip_punctuation',\n",
        "#     split='whitespace',\n",
        "#     ngrams=None,\n",
        "#     max_tokens=10000,\n",
        "#     output_mode='int',\n",
        "#     output_sequence_length=512,\n",
        "#     pad_to_max_tokens=True)\n",
        "# vectorize_layer.adapt(X_train)\n",
        "# x_train = vectorize_layer(X_train) \n",
        "# x_test = vectorize_layer(X_test) \n",
        "\n",
        "# Embed each integer in a 100-dimensional vector\n",
        "x = tf.keras.layers.Embedding(10000, 100)(inputs) # top 10k\n",
        "\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=1) # , validation_data=(x_test, y_test)\n",
        "\n",
        "print(model.metrics_names)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "wxJAv7s0fvDA"
      },
      "id": "wxJAv7s0fvDA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pIjoa0Qh8aF"
      },
      "id": "7pIjoa0Qh8aF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}